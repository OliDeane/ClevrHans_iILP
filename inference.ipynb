{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswip import Prolog\n",
    "from inference import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-process text file\n",
    "* remove the \\n\n",
    "* remove the preceeding white space\n",
    "* combine current string with the next string if i+1 does not begin with true_class\n",
    "'''\n",
    "\n",
    "# Combine lines and clean strings\n",
    "def clean_theory(theory):\n",
    "    clean_theory = []\n",
    "    for i in range(0,len(theory)-1):\n",
    "        if ':-' not in theory[i+1]:\n",
    "                rule = theory[i].strip().replace('\\n', '') + ' ' + theory[i+1].strip().replace('\\n', '')\n",
    "                clean_theory.append(rule)\n",
    "    \n",
    "    return clean_theory\n",
    "\n",
    "def merge_lines(clean_theory):\n",
    "    # Tidy up to ensure all rules begin with head :- body\n",
    "    full_theory = []\n",
    "    if len(clean_theory) > 1:\n",
    "        for i in range(0,len(clean_theory)-1):\n",
    "            if ':-' not in clean_theory[i+1]:\n",
    "                    rule = clean_theory[i] + ' ' + clean_theory[i+1]\n",
    "                    full_theory.append(rule)\n",
    "            elif ':-' in clean_theory[i]:\n",
    "                full_theory.append(clean_theory[i])\n",
    "\n",
    "    else:\n",
    "        full_theory = clean_theory\n",
    "\n",
    "    return full_theory\n",
    "\n",
    "def add_variable(full_theory):\n",
    "    new_theory = []\n",
    "    for rule in full_theory:\n",
    "        head = 'true_class(A,EX) :-'\n",
    "        body = rule.rpartition(':-')[2][:-1]\n",
    "\n",
    "        new_body = body + ', Ex = ' + f'({body}).'\n",
    "        new_clause = head + new_body\n",
    "\n",
    "        new_theory.append(new_clause)\n",
    "    \n",
    "    return new_theory\n",
    "    \n",
    "def save_ruleset_to_prolog(dataset, filename, full_theory):\n",
    "    file = open(filename, 'w')\n",
    "    file.write(f':-consult(\"aleph_input/{dataset}_aleph.bk\").\\n')\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "    for rule in full_theory:\n",
    "        head = 'true_class(A,Ex) :-'\n",
    "        body = rule.rpartition(':-')[2][:-1]\n",
    "\n",
    "        file.write(head + \"\\n\")\n",
    "        file.write(\"    \" + body + \",\\n\")\n",
    "        file.write(\"    Ex = \" + f\"[{body.strip()}].\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "    file.close()        \n",
    "\n",
    "def translate_theory(dataset, filename = 'working_theory.pl'):\n",
    "\n",
    "    with open(f'{dataset}_theory.txt') as f:\n",
    "        theory = f.readlines()\n",
    "    \n",
    "    theory = clean_theory(theory)\n",
    "    theory = merge_lines(theory)\n",
    "    save_ruleset_to_prolog(dataset, filename, theory)\n",
    "\n",
    "def single_instance_inference(dataset, example_number, prolog):\n",
    "    translate_theory(dataset = dataset)\n",
    "    prolog.consult(\"working_theory.pl\")\n",
    "    result = list(prolog.query(f\"true_class(example_{example_number}, Explanation)\"))\n",
    "\n",
    "    # Extract prediction and explanation\n",
    "    if len(result) == 0:\n",
    "        pred = 0\n",
    "        explanation = None\n",
    "    elif len(result) == 1:\n",
    "        pred = 1\n",
    "        explanation = result[0]['Explanation']\n",
    "    else: # need to fix this - why is some len 1 and others more?\n",
    "        pred = 1\n",
    "        explanation = result[0]['Explanation']\n",
    "\n",
    "    return (pred, explanation)\n",
    "\n",
    "def add_constraint(dataset):\n",
    "    output_directory = 'aleph_input'\n",
    "    bk_file = open(output_directory + '/' + dataset + '_aleph.bk', 'a')\n",
    "\n",
    "    bk_file.write(\":-consult('constraints.pl').\\n\")\n",
    "    bk_file.write(\"bodyList(Body, FinalList) :-\\n \\\n",
    "        clause2list(Body,[],Output, Clause), list_to_term(Clause, Term), insertAtEnd(Term,Output,FinalList).\\n\")\n",
    "    bk_file.write(\"false :- \\n\\\n",
    "        hypothesis(_,Body,_), bodyList(Body, List), !, member(has_color(_,_), List).\")\n",
    "    \n",
    "\n",
    "\n",
    "    bk_file.close()\n",
    "\n",
    "def ilp_induce(dataset, prolog):\n",
    "\n",
    "    # generate initial ILP thoery and save to a file\n",
    "    prolog.consult('aleph6.pl')\n",
    "    list(prolog.query(f\"read_all('aleph_input/{dataset}_aleph').\"))\n",
    "    list(prolog.query(\"induce.\"))\n",
    "    list(prolog.query(f\"write_rules('{dataset}_theory.txt').\"))\n",
    "\n",
    "# Below are in the GUI_utils file\n",
    "\n",
    "def delete_existing_constraints(dataset='hans'):\n",
    "    '''Currently stored within GUI_utils'''\n",
    "    lines = []\n",
    "    path = f'aleph_input/{dataset}_aleph.bk'\n",
    "    with open(path, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    \n",
    "    # write file\n",
    "    with open(path, 'w') as fp:\n",
    "        for line in lines:\n",
    "            if line[0:5] != 'false' and line != \":-consult('constraints.pl).\\n\":\n",
    "                fp.write(line)\n",
    "\n",
    "\n",
    "def transform_clause(og_clause):\n",
    "    '''Transforms clause to natural language'''\n",
    "\n",
    "    clause = list(set(og_clause))\n",
    "\n",
    "    # find ALL THE first contains predicate\n",
    "    contains_preds = [i for i in clause if 'contains' in i]\n",
    "    nl_clause = ''\n",
    "    for object_predicate in contains_preds:\n",
    "        # add an \"image contains an object X\" sentence\n",
    "        clause.remove(object_predicate)\n",
    "        var = object_predicate.rpartition('contains(')[2].rpartition(', ')[0]\n",
    "        nl_clause = nl_clause + f\"Image contains an object {var}\"\n",
    "\n",
    "        # search for the elements in the clause containing that variable\n",
    "        # for each element: add the attribute and the attribute value\n",
    "        attribute_preds = [i for i in clause if var in i]\n",
    "        for idx, predicate in enumerate(attribute_preds):\n",
    "            attribute = predicate.rpartition('('+var)[0].rpartition('has_')[2]\n",
    "            att_value = predicate.rpartition(var+', ')[2][:-1]\n",
    "\n",
    "            if len(attribute_preds) > 1 and idx == 0:\n",
    "                nl_clause = nl_clause + f\" with {attribute} {att_value}\"\n",
    "            elif len(attribute_preds) == 1 and idx ==0:\n",
    "                nl_clause = nl_clause + f\" with {attribute} {att_value}. \"\n",
    "            elif idx+1 == len(attribute_preds) and idx > 0:\n",
    "                nl_clause = nl_clause + f\" and {attribute} {att_value}.\\n\"\n",
    "            else:\n",
    "                nl_clause = nl_clause + f\" and {attribute} {att_value}\"\n",
    "            \n",
    "    return nl_clause\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial theory with prediction\n",
    "\n",
    "dataset = 'hans'\n",
    "prolog = Prolog()\n",
    "ilp_induce(dataset, prolog)\n",
    "result1 = single_instance_inference(dataset=dataset, example_number=3, prolog=prolog)\n",
    "pred, explanation = result1\n",
    "\n",
    "# # Add constraint and re-train ILP program\n",
    "add_constraint(dataset)\n",
    "ilp_induce(dataset, prolog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, ['contains(oid_91, example_3)', 'has_shape(oid_91, cylinder)', 'contains(oid_1, example_3)', 'has_shape(oid_1, cube)', 'contains(oid_91, example_3)', 'has_shape(oid_91, cylinder)', 'contains(oid_1, example_3)', 'has_shape(oid_1, cube)', 'has_color(oid_1, gray)']) \n",
      "\n",
      " (1, ['contains(oid_91, example_3)', 'has_shape(oid_91, cylinder)', 'has_size(oid_91, large)', 'contains(oid_14, example_3)', 'contains(oid_91, example_3)', 'has_shape(oid_91, cylinder)', 'has_size(oid_91, large)', 'contains(oid_14, example_3)', 'has_shape(oid_14, cube)'])\n"
     ]
    }
   ],
   "source": [
    "# Show results with/out new added constraint\n",
    "result2 = single_instance_inference(dataset=dataset, example_number=3, prolog=prolog)\n",
    "print(result1, '\\n\\n',result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranform clasues to natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause = ['contains(oid_69, example_1)', 'has_shape(oid_69, cylinder)', 'contains(oid_17, example_1)', 'has_shape(oid_17, cube)', 'contains(oid_69, example_1)', 'has_shape(oid_69, cylinder)', 'contains(oid_17, example_1)', 'has_shape(oid_17, cube)', 'has_color(oid_17, gray)'] \n",
    "\n",
    "nl_clause = \"image containts an object 1 with shape cyclindar. Image contains object 2\\\n",
    "    with shape cube and color gray.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Image contains an object oid_69 with shape cylinder. Image contains an object oid_17 with color gray and shape cube.\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clause = ['contains(oid_69, example_1)', 'has_shape(oid_69, cylinder)', 'contains(oid_17, example_1)', 'has_shape(oid_17, cube)', 'contains(oid_69, example_1)', 'has_shape(oid_69, cylinder)', 'contains(oid_17, example_1)', 'has_shape(oid_17, cube)', 'has_color(oid_17, gray)'] \n",
    "nl = transform_clause(clause)\n",
    "nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "# read file\n",
    "with open(r\"E:\\demos\\files\\sample.txt\", 'r') as fp:\n",
    "    # read an store all lines into list\n",
    "    lines = fp.readlines()\n",
    "\n",
    "# Write file\n",
    "with open(r\"E:\\demos\\files\\sample.txt\", 'w') as fp:\n",
    "    # iterate each line\n",
    "    for number, line in enumerate(lines):\n",
    "        # delete line 5 and 8. or pass any Nth line you want to remove\n",
    "        # note list index starts from 0\n",
    "        if number not in [4, 7]:\n",
    "            fp.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 ('ilp_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a438c5a9d814fdcf4c71bfa51aee82b7b3f77bdca39ab43f22c1c3326f23fe8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
