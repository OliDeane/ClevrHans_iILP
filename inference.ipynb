{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswip import Prolog\n",
    "from inference import *\n",
    "from PIL import ImageTk, Image\n",
    "from program_generation.norel_program import *\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current problems\n",
    "\n",
    "* Theory includes repeat predicates for some reason\n",
    "\n",
    "* The test instances are coming up negative after the constraint is added. Very sad. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-process text file\n",
    "* remove the \\n\n",
    "* remove the preceeding white space\n",
    "* combine current string with the next string if i+1 does not begin with true_class\n",
    "'''\n",
    "\n",
    "# Combine lines and clean strings\n",
    "def clean_theory(theory):\n",
    "    clean_theory = []\n",
    "    for i in range(0,len(theory)-1):\n",
    "        if ':-' not in theory[i+1]:\n",
    "                rule = theory[i].strip().replace('\\n', '') + ' ' + theory[i+1].strip().replace('\\n', '')\n",
    "                clean_theory.append(rule)\n",
    "    \n",
    "    return clean_theory\n",
    "\n",
    "def merge_lines(clean_theory):\n",
    "    # Tidy up to ensure all rules begin with head :- body\n",
    "    full_theory = []\n",
    "    if len(clean_theory) > 1:\n",
    "        for i in range(0,len(clean_theory)-1):\n",
    "            if ':-' not in clean_theory[i+1]:\n",
    "                    rule = clean_theory[i] + ' ' + clean_theory[i+1]\n",
    "                    full_theory.append(rule)\n",
    "            elif ':-' in clean_theory[i]:\n",
    "                full_theory.append(clean_theory[i])\n",
    "\n",
    "    else:\n",
    "        full_theory = clean_theory\n",
    "\n",
    "    return list(set(full_theory))\n",
    "\n",
    "def add_variable(full_theory):\n",
    "    new_theory = []\n",
    "    for rule in full_theory:\n",
    "        head = 'true_class(A,EX) :-'\n",
    "        body = rule.rpartition(':-')[2][:-1]\n",
    "\n",
    "        new_body = body + ', Ex = ' + f'({body}).'\n",
    "        new_clause = head + new_body\n",
    "\n",
    "        new_theory.append(new_clause)\n",
    "    \n",
    "    return new_theory\n",
    "    \n",
    "def save_ruleset_to_prolog(dataset, filename, full_theory):\n",
    "    file = open(filename, 'w')\n",
    "    file.write(f\":-consult('aleph_input/{dataset}_aleph.bk').\\n\")\n",
    "    # file.write(f':-consult(\"test_samples.pl\").\\n')\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "    for rule in full_theory:\n",
    "        head = 'true_class(A,Ex) :-'\n",
    "        body = rule.rpartition(':-')[2][:-1]\n",
    "\n",
    "        file.write(head + \"\\n\")\n",
    "        file.write(\"    \" + body + \",\\n\")\n",
    "        file.write(\"    Ex = \" + f\"[{body.strip()}].\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "    file.close()        \n",
    "\n",
    "def translate_theory(dataset, filename = 'working_theory.pl'):\n",
    "\n",
    "    with open(f'{dataset}_theory.txt') as f:\n",
    "        theory = f.readlines()\n",
    "    \n",
    "    theory = clean_theory(theory)\n",
    "    theory = merge_lines(theory)\n",
    "    save_ruleset_to_prolog(dataset, filename, theory)\n",
    "\n",
    "def single_instance_inference(dataset, example_number, prolog):\n",
    "    translate_theory(dataset = dataset)\n",
    "    prolog.consult(\"working_theory.pl\")\n",
    "    result = list(prolog.query(f\"true_class(example_{example_number}, Explanation)\"))\n",
    "\n",
    "    # Extract prediction and explanation\n",
    "    if len(result) == 0:\n",
    "        pred = 0\n",
    "        explanation = None\n",
    "    elif len(result) == 1:\n",
    "        pred = 1\n",
    "        explanation = result[0]['Explanation']\n",
    "    else: # need to fix this - why is some len 1 and others more?\n",
    "        pred = 1\n",
    "        explanation = result[0]['Explanation']\n",
    "\n",
    "    return (pred, explanation)\n",
    "\n",
    "def add_constraint(dataset):\n",
    "    output_directory = 'aleph_input'\n",
    "    bk_file = open(output_directory + '/' + dataset + '_aleph.bk', 'a')\n",
    "\n",
    "    bk_file.write(\":-consult('constraints.pl').\\n\")\n",
    "    bk_file.write(\"bodyList(Body, FinalList) :-\\n \\\n",
    "        clause2list(Body,[],Output, Clause), list_to_term(Clause, Term), insertAtEnd(Term,Output,FinalList).\\n\")\n",
    "    bk_file.write(\"false :- \\n\\\n",
    "        hypothesis(_,Body,_), bodyList(Body, List), !, member(has_color(_,_), List).\")\n",
    "    \n",
    "\n",
    "\n",
    "    bk_file.close()\n",
    "\n",
    "def ilp_induce(dataset, prolog):\n",
    "\n",
    "    # generate initial ILP thoery and save to a file\n",
    "    prolog.consult('aleph6.pl')\n",
    "    list(prolog.query(f\"read_all('aleph_input/{dataset}_aleph').\"))\n",
    "    list(prolog.query(\"induce.\"))\n",
    "    list(prolog.query(f\"write_rules('{dataset}_theory.txt').\"))\n",
    "\n",
    "# Below are in the GUI_utils file\n",
    "\n",
    "def delete_existing_constraints(dataset='hans'):\n",
    "    '''Currently stored within GUI_utils'''\n",
    "    lines = []\n",
    "    path = f'aleph_input/{dataset}_aleph.bk'\n",
    "    with open(path, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    \n",
    "    # write file\n",
    "    with open(path, 'w') as fp:\n",
    "        for line in lines:\n",
    "            if line[0:5] != 'false' and line != \":-consult('constraints.pl).\\n\":\n",
    "                fp.write(line)\n",
    "\n",
    "def transform_clause(og_clause):\n",
    "    '''Transforms clause to natural language'''\n",
    "\n",
    "    clause = list(set(og_clause))\n",
    "\n",
    "    # find ALL THE first contains predicate\n",
    "    contains_preds = [i for i in clause if 'contains' in i]\n",
    "    nl_clause = ''\n",
    "    for object_predicate in contains_preds:\n",
    "        # add an \"image contains an object X\" sentence\n",
    "        clause.remove(object_predicate)\n",
    "        var = object_predicate.rpartition('contains(')[2].rpartition(', ')[0]\n",
    "        nl_clause = nl_clause + f\"Image contains an object {var}\"\n",
    "\n",
    "        # search for the elements in the clause containing that variable\n",
    "        # for each element: add the attribute and the attribute value\n",
    "        attribute_preds = [i for i in clause if var in i]\n",
    "        for idx, predicate in enumerate(attribute_preds):\n",
    "            attribute = predicate.rpartition('('+var)[0].rpartition('has_')[2]\n",
    "            att_value = predicate.rpartition(var+', ')[2][:-1]\n",
    "\n",
    "            if len(attribute_preds) > 1 and idx == 0:\n",
    "                nl_clause = nl_clause + f\" with {attribute} {att_value}\"\n",
    "            elif len(attribute_preds) == 1 and idx ==0:\n",
    "                nl_clause = nl_clause + f\" with {attribute} {att_value}. \"\n",
    "            elif idx+1 == len(attribute_preds) and idx > 0:\n",
    "                nl_clause = nl_clause + f\" and {attribute} {att_value}.\\n\"\n",
    "            else:\n",
    "                nl_clause = nl_clause + f\" and {attribute} {att_value}\"\n",
    "            \n",
    "    return nl_clause\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test initial prolog inference with constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial theory with prediction\n",
    "\n",
    "dataset = 'hans'\n",
    "prolog = Prolog()\n",
    "ilp_induce(dataset, prolog)\n",
    "result1 = single_instance_inference(dataset=dataset, example_number=1, prolog=prolog)\n",
    "pred, explanation = result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add constraint and re-train ILP program\n",
    "add_constraint(dataset)\n",
    "ilp_induce(dataset, prolog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, ['contains(oid_91, example_3)', 'has_shape(oid_91, cylinder)', 'contains(oid_1, example_3)', 'has_shape(oid_1, cube)', 'contains(oid_91, example_3)', 'has_shape(oid_91, cylinder)', 'contains(oid_1, example_3)', 'has_shape(oid_1, cube)', 'has_color(oid_1, gray)']) \n",
      "\n",
      " (1, ['contains(oid_91, example_3)', 'has_shape(oid_91, cylinder)', 'has_size(oid_91, large)', 'contains(oid_14, example_3)', 'contains(oid_91, example_3)', 'has_shape(oid_91, cylinder)', 'has_size(oid_91, large)', 'contains(oid_14, example_3)', 'has_shape(oid_14, cube)'])\n"
     ]
    }
   ],
   "source": [
    "# Show results with/out new added constraint\n",
    "result2 = single_instance_inference(dataset=dataset, example_number=3, prolog=prolog)\n",
    "print(result1, '\\n\\n',result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranform clauses to natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Image contains an object oid_69 with shape cylinder. Image contains an object oid_17 with color gray and shape cube.\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clause = ['contains(oid_69, example_1)', 'has_shape(oid_69, cylinder)', 'contains(oid_17, example_1)', 'has_shape(oid_17, cube)', 'contains(oid_69, example_1)', 'has_shape(oid_69, cylinder)', 'contains(oid_17, example_1)', 'has_shape(oid_17, cube)', 'has_color(oid_17, gray)'] \n",
    "nl = transform_clause(clause)\n",
    "nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "# read file\n",
    "with open(r\"E:\\demos\\files\\sample.txt\", 'r') as fp:\n",
    "    # read an store all lines into list\n",
    "    lines = fp.readlines()\n",
    "\n",
    "# Write file\n",
    "with open(r\"E:\\demos\\files\\sample.txt\", 'w') as fp:\n",
    "    # iterate each line\n",
    "    for number, line in enumerate(lines):\n",
    "        # delete line 5 and 8. or pass any Nth line you want to remove\n",
    "        # note list index starts from 0\n",
    "        if number not in [4, 7]:\n",
    "            fp.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'Image contains an object oid_69 with shape cylinder. Image contains an object oid_17 with shape cube and color gray.\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = 'hans'\n",
    "prolog = Prolog()\n",
    "\n",
    "result = single_instance_inference(dataset=dataset, example_number=1, prolog=prolog)\n",
    "pred, explanation = result\n",
    "\n",
    "if explanation:\n",
    "    explanation = transform_clause(explanation)\n",
    "\n",
    "pred, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# model_path = \"./trained_model/mask_rcnn_clevr_0030_allclasses.h5\"\n",
    "# config = InferenceConfig()\n",
    "model = load_model(os.getcwd(), config = InferenceConfig(), model_path = \"./trained_model/mask_rcnn_clevr_0030_allclasses.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image and run inference\n",
    "\n",
    "img_id = 1\n",
    "image_filename = f\"./GUI_interface/demo_images/image_{img_id}.png\"\n",
    "\n",
    "dataset = 'hans'\n",
    "image = skimage.io.imread(image_filename)\n",
    "image = image[:,:,:3]\n",
    "results = model.detect([image], verbose=0)\n",
    "r = results[0]\n",
    "\n",
    "plt.imshow(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formalise general output.\n",
    "# Do not include ilp class here - as this is for inference.\n",
    "\n",
    "shape_categories, material_categories, color_categories,\\\n",
    "        size_categories, class_names = define_object_types()\n",
    "img_objects = ([class_names[r['class_ids'][i]] for i in range(len(r['class_ids']))])\n",
    "\n",
    "rois = r['rois'].tolist()\n",
    "\n",
    "# Can ignore for now - we're not using relational predicates in demo yet. \n",
    "centroids = {}\n",
    "centroids['X'] = [(roi[1] + roi[3])/2 for roi in rois]\n",
    "centroids['Y'] = [(roi[0] + roi[2])/2 for roi in rois]\n",
    "\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            class_names, r['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate program from MRCNN Output\n",
    "\n",
    "# First generate all possible objects so we can ID them later\n",
    "attribute_dict = {'color': list(color_categories.keys()), 'material': list(material_categories.keys()),\n",
    "                        'size': list(size_categories.keys()), 'shape': list(shape_categories.keys())}\n",
    "full_oblist = get_all_objects(attribute_dict)\n",
    "\n",
    "# create test file if doesn't already exist and open\n",
    "# For now - all test instances go into the bk file\n",
    "\n",
    "# if not os.path.exists('test_samples.pl'):\n",
    "#     bk_file = open(f\"aleph_input/{dataset}_aleph.bk\",'a')\n",
    "#     bk_file.write(f\":-consult('test_samples.pl').\\n\\n\")\n",
    "#     bk_file.close()\n",
    "# test_file = open(f\"test_samples.pl\",'a+')\n",
    "\n",
    "bk_file = open(f\"aleph_input/{dataset}_aleph.bk\",'a')\n",
    "bk_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "# generate an example_id - maybe via the image number provided\n",
    "example_id = str(int(img_id) + 200)\n",
    "\n",
    "# add image objects contained\n",
    "\n",
    "for object in img_objects:\n",
    "    shape, material, color, size = object.split()\n",
    "\n",
    "    # find the index id of the object recognised\n",
    "    object_idx = full_oblist.index([shape, material, color, size])\n",
    "    object_id = f'oid_{object_idx}'\n",
    "    bk_file.write(f\"contains({object_id}, example_{example_id}).\\n\")\n",
    "\n",
    "bk_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'Image contains an object oid_1 with shape cube and color gray.\\nImage contains an object oid_93 with shape cylinder. ')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'hans'\n",
    "prolog = Prolog()\n",
    "\n",
    "result = single_instance_inference(dataset=dataset, example_number=201, prolog=prolog)\n",
    "pred, explanation = result\n",
    "\n",
    "if explanation:\n",
    "    explanation = transform_clause(explanation)\n",
    "\n",
    "pred, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 ('ilp_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a438c5a9d814fdcf4c71bfa51aee82b7b3f77bdca39ab43f22c1c3326f23fe8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
